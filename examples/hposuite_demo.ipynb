{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook with Minimal example to run hposuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ex_Functional_Bench': <hpoglue.benchmark.FunctionalBenchmark object at 0x7ef7d6e9acb0>, 'Ex_Surrogate_Bench': BenchmarkDescription(name='Ex_Surrogate_Bench', config_space=Configuration space object:\n",
      "  Hyperparameters:\n",
      "    lr_decay_factor, Type: UniformFloat, Range: [0.010093, 0.989012], Default: 0.4995525\n",
      "    lr_initial, Type: UniformFloat, Range: [1e-05, 9.779176], Default: 0.0098889716351, on log-scale\n",
      "    lr_power, Type: UniformFloat, Range: [0.100708, 1.999376], Default: 1.050042\n",
      "    opt_momentum, Type: UniformFloat, Range: [5.9e-05, 0.998993], Default: 0.0076772773169, on log-scale\n",
      ", load=functools.partial(<function _get_surrogate_benchmark at 0x7ef7d6e91b40>, benchmark_name='cifar100_wideresnet_2048', datadir=PosixPath('/home/soham/repos/hposuite/data/pd1')), metrics={'valid_error_rate': Measure(minimize=True, kind=<Kind.METRIC: 'metric'>, bounds=(0, 1))}, test_metrics=None, costs={'train_cost': Measure(minimize=True, kind=<Kind.COST: 'cost'>, bounds=(0, inf))}, fidelities={'epoch': RangeFidelity(kind=<class 'int'>, min=1, max=199, stepsize=1, supports_continuation=True)}, has_conditionals=False, is_tabular=False, env=Env(name='py310-mfpbench-1.9-pd1'), mem_req_mb=12288), 'mfh3_bad': BenchmarkDescription(name='mfh3_bad', config_space=Configuration space object:\n",
      "  Hyperparameters:\n",
      "mfh3_bad\n",
      "    X_0, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      "    X_1, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      "    X_2, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      ", load=functools.partial(<function _get_surrogate_benchmark at 0x7ef7d6e91e10>, benchmark_name='mfh3_bad'), metrics={'value': Measure(minimize=True, kind=<Kind.METRIC: 'metric'>, bounds=(-3.32237, inf))}, test_metrics=None, costs={'fid_cost': Measure(minimize=True, kind=<Kind.COST: 'cost'>, bounds=(0.05, 1))}, fidelities={'z': RangeFidelity(kind=<class 'int'>, min=1, max=100, stepsize=1, supports_continuation=True)}, has_conditionals=False, is_tabular=False, env=Env(name='py310-mfpbench-1.9-mfh'), mem_req_mb=1024), 'mfh6_bad': BenchmarkDescription(name='mfh6_bad', config_space=Configuration space object:\n",
      "  Hyperparameters:\n",
      "mfh6_bad\n",
      "    X_0, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      "    X_1, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      "    X_2, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      "    X_3, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      "    X_4, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      "    X_5, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      ", load=functools.partial(<function _get_surrogate_benchmark at 0x7ef7d6e91e10>, benchmark_name='mfh6_bad'), metrics={'value': Measure(minimize=True, kind=<Kind.METRIC: 'metric'>, bounds=(-3.86278, inf))}, test_metrics=None, costs={'fid_cost': Measure(minimize=True, kind=<Kind.COST: 'cost'>, bounds=(0.05, 1))}, fidelities={'z': RangeFidelity(kind=<class 'int'>, min=1, max=100, stepsize=1, supports_continuation=True)}, has_conditionals=False, is_tabular=False, env=Env(name='py310-mfpbench-1.9-mfh'), mem_req_mb=1024), 'mfh3_good': BenchmarkDescription(name='mfh3_good', config_space=Configuration space object:\n",
      "  Hyperparameters:\n",
      "mfh3_good\n",
      "    X_0, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      "    X_1, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      "    X_2, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      ", load=functools.partial(<function _get_surrogate_benchmark at 0x7ef7d6e91e10>, benchmark_name='mfh3_good'), metrics={'value': Measure(minimize=True, kind=<Kind.METRIC: 'metric'>, bounds=(-3.32237, inf))}, test_metrics=None, costs={'fid_cost': Measure(minimize=True, kind=<Kind.COST: 'cost'>, bounds=(0.05, 1))}, fidelities={'z': RangeFidelity(kind=<class 'int'>, min=1, max=100, stepsize=1, supports_continuation=True)}, has_conditionals=False, is_tabular=False, env=Env(name='py310-mfpbench-1.9-mfh'), mem_req_mb=1024), 'mfh6_good': BenchmarkDescription(name='mfh6_good', config_space=Configuration space object:\n",
      "  Hyperparameters:\n",
      "mfh6_good\n",
      "    X_0, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      "    X_1, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      "    X_2, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      "    X_3, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      "    X_4, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      "    X_5, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      ", load=functools.partial(<function _get_surrogate_benchmark at 0x7ef7d6e91e10>, benchmark_name='mfh6_good'), metrics={'value': Measure(minimize=True, kind=<Kind.METRIC: 'metric'>, bounds=(-3.86278, inf))}, test_metrics=None, costs={'fid_cost': Measure(minimize=True, kind=<Kind.COST: 'cost'>, bounds=(0.05, 1))}, fidelities={'z': RangeFidelity(kind=<class 'int'>, min=1, max=100, stepsize=1, supports_continuation=True)}, has_conditionals=False, is_tabular=False, env=Env(name='py310-mfpbench-1.9-mfh'), mem_req_mb=1024), 'mfh3_moderate': BenchmarkDescription(name='mfh3_moderate', config_space=Configuration space object:\n",
      "  Hyperparameters:\n",
      "mfh3_moderate\n",
      "    X_0, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      "    X_1, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      "    X_2, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      ", load=functools.partial(<function _get_surrogate_benchmark at 0x7ef7d6e91e10>, benchmark_name='mfh3_moderate'), metrics={'value': Measure(minimize=True, kind=<Kind.METRIC: 'metric'>, bounds=(-3.32237, inf))}, test_metrics=None, costs={'fid_cost': Measure(minimize=True, kind=<Kind.COST: 'cost'>, bounds=(0.05, 1))}, fidelities={'z': RangeFidelity(kind=<class 'int'>, min=1, max=100, stepsize=1, supports_continuation=True)}, has_conditionals=False, is_tabular=False, env=Env(name='py310-mfpbench-1.9-mfh'), mem_req_mb=1024), 'mfh6_moderate': BenchmarkDescription(name='mfh6_moderate', config_space=Configuration space object:\n",
      "  Hyperparameters:\n",
      "mfh6_moderate\n",
      "    X_0, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      "    X_1, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      "    X_2, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      "    X_3, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      "    X_4, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      "    X_5, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      ", load=functools.partial(<function _get_surrogate_benchmark at 0x7ef7d6e91e10>, benchmark_name='mfh6_moderate'), metrics={'value': Measure(minimize=True, kind=<Kind.METRIC: 'metric'>, bounds=(-3.86278, inf))}, test_metrics=None, costs={'fid_cost': Measure(minimize=True, kind=<Kind.COST: 'cost'>, bounds=(0.05, 1))}, fidelities={'z': RangeFidelity(kind=<class 'int'>, min=1, max=100, stepsize=1, supports_continuation=True)}, has_conditionals=False, is_tabular=False, env=Env(name='py310-mfpbench-1.9-mfh'), mem_req_mb=1024), 'mfh3_terrible': BenchmarkDescription(name='mfh3_terrible', config_space=Configuration space object:\n",
      "  Hyperparameters:\n",
      "mfh3_terrible\n",
      "    X_0, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      "    X_1, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      "    X_2, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      ", load=functools.partial(<function _get_surrogate_benchmark at 0x7ef7d6e91e10>, benchmark_name='mfh3_terrible'), metrics={'value': Measure(minimize=True, kind=<Kind.METRIC: 'metric'>, bounds=(-3.32237, inf))}, test_metrics=None, costs={'fid_cost': Measure(minimize=True, kind=<Kind.COST: 'cost'>, bounds=(0.05, 1))}, fidelities={'z': RangeFidelity(kind=<class 'int'>, min=1, max=100, stepsize=1, supports_continuation=True)}, has_conditionals=False, is_tabular=False, env=Env(name='py310-mfpbench-1.9-mfh'), mem_req_mb=1024), 'mfh6_terrible': BenchmarkDescription(name='mfh6_terrible', config_space=Configuration space object:\n",
      "  Hyperparameters:\n",
      "mfh6_terrible\n",
      "    X_0, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      "    X_1, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      "    X_2, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      "    X_3, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      "    X_4, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      "    X_5, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
      ", load=functools.partial(<function _get_surrogate_benchmark at 0x7ef7d6e91e10>, benchmark_name='mfh6_terrible'), metrics={'value': Measure(minimize=True, kind=<Kind.METRIC: 'metric'>, bounds=(-3.86278, inf))}, test_metrics=None, costs={'fid_cost': Measure(minimize=True, kind=<Kind.COST: 'cost'>, bounds=(0.05, 1))}, fidelities={'z': RangeFidelity(kind=<class 'int'>, min=1, max=100, stepsize=1, supports_continuation=True)}, has_conditionals=False, is_tabular=False, env=Env(name='py310-mfpbench-1.9-mfh'), mem_req_mb=1024)}\n",
      "{'RandomSearch': <class 'lib.optimizers.random_search.RandomSearch'>, 'Ex_Multifidelity_Opt': <class 'lib.optimizers.ex_multifidelity_opt.Ex_Multifidelity_Opt'>, 'Ex_Blackbox_Opt': <class 'lib.optimizers.ex_blackbox_opt.Ex_Blackbox_Opt'>}\n"
     ]
    }
   ],
   "source": [
    "# It is assumed that the datadir is hposuite/data\n",
    "\n",
    "from lib.benchmarks import BENCHMARKS\n",
    "from lib.optimizers import OPTIMIZERS\n",
    "\n",
    "print(BENCHMARKS)\n",
    "print(OPTIMIZERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Random Search on a Functional Benchmark (eg: ACKLEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_benchmark = BENCHMARKS[\"Ex_Functional_Bench\"]\n",
    "rs = OPTIMIZERS[\"RandomSearch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hposuite import create_study\n",
    "\n",
    "study = create_study(\n",
    "    name=\"hposuite_demo_rs_func\",\n",
    "    output_dir=\"../../hposuite-output\",\n",
    "    optimizers=rs,\n",
    "    benchmarks=func_benchmark,\n",
    "    num_seeds=5,\n",
    "    budget=100,\n",
    ")\n",
    "\n",
    "study.optimize(overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a BlackBox Optimizer (eg: SMAC BlackBoxFacade) on a Synthetic Benchmark (eg: MF Hartmann 3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_optimizer = OPTIMIZERS[\"Ex_Blackbox_Opt\"]\n",
    "mf_syn_benchmark = BENCHMARKS[\"mfh3_good\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hposuite import create_study\n",
    "\n",
    "study = create_study(\n",
    "    name=\"hposuite_demo_bb_mfsyn\",\n",
    "    output_dir=\"../../hposuite-output\",\n",
    "    optimizers=bb_optimizer,\n",
    "    benchmarks=mf_syn_benchmark,\n",
    "    num_seeds=5,\n",
    "    budget=20,\n",
    ")\n",
    "\n",
    "study.optimize(overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a MultiFidelity Optimizer (eg: Hyperband) on a Surrogate Benchmark (eg: PD1 Cifar100 Wide Resnet 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_benchmark = BENCHMARKS[\"Ex_Surrogate_Bench\"]\n",
    "mf_optimizer = OPTIMIZERS[\"Ex_Multifidelity_Opt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hposuite import create_study\n",
    "\n",
    "study = create_study(\n",
    "    name=\"hposuite_demo_sg_mf\",\n",
    "    output_dir=\"../../hposuite-output\",\n",
    "    optimizers=mf_optimizer,\n",
    "    benchmarks=sg_benchmark,\n",
    "    num_seeds=5,\n",
    "    budget=100,\n",
    ")\n",
    "\n",
    "study.optimize(overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hposuite_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
